{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAN-A2C 方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# 策略网络\n",
    "class PolicyNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.sigmoid(self.fc1(x))\n",
    "        return F.softmax(self.fc2(x), dim=1)\n",
    "\n",
    "\n",
    "# 价值网络    \n",
    "class ValueNet(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, state_dim, hidden_dim):\n",
    "        super(ValueNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "       \n",
    "class MutiActorCritic:\n",
    " \n",
    "    def __init__(self, agent_n, observation_dim, state_dim, hidden_dim, action_dim, actor_lr, critic_lr, tau, gamma, device):\n",
    "        self.agent_n = agent_n\n",
    "        self.actors = [PolicyNet(observation_dim, hidden_dim, action_dim).to(device) for _ in range(agent_n)] # 策略网络\n",
    "        self.critics = [ValueNet(state_dim, hidden_dim).to(device) for _ in range(agent_n)] # 价值网络\n",
    "        self.target_critics = [ValueNet(state_dim, hidden_dim).to(device) for _ in range(agent_n)] # 目标网络\n",
    "        self.actor_optimizers = [torch.optim.Adam(actor.parameters(), lr=actor_lr) for actor in self.actors] # 策略网络优化器\n",
    "        self.critic_optimizers = [torch.optim.Adam(critic.parameters(), lr=critic_lr) for critic in self.critics] # 价值网络优化器\n",
    "        self.actor_schedulers =  [torch.optim.lr_scheduler.StepLR(optimizer, 100, gamma=0.5, last_epoch=-1) for optimizer in self.actor_optimizers] # 学习率优化器\n",
    "        self.critic_schedulers =  [torch.optim.lr_scheduler.StepLR(optimizer, 100, gamma=0.5, last_epoch=-1) for optimizer in self.critic_optimizers]\n",
    "        self.tau = tau  # 目标网络软更新参数\n",
    "        self.gamma = gamma\n",
    "        self.device = device\n",
    "\n",
    "    def take_action(self, observations):\n",
    "        observations_tensor = [torch.tensor([observations[i]], dtype=torch.float).to(self.device) for i in range(self.agent_n)] # 转化为tensor\n",
    "        probs = [self.actors[i](observations_tensor[i]) for i in range(self.agent_n)] # 动作概率\n",
    "        actions = [prob.tolist()[0] for prob in probs]\n",
    "        return actions\n",
    "    \n",
    "    def soft_update(self, net, target_net):\n",
    "        for param_target, param in zip(target_net.parameters(), net.parameters()):\n",
    "            param_target.data.copy_(param_target.data * (1.0 - self.tau) + param.data * self.tau)\n",
    "        \n",
    "    def actor_update(self, agent_i, transition_dict):\n",
    "        observations = torch.tensor(transition_dict['observations'], dtype=torch.float).to(self.device)\n",
    "        states = torch.tensor(transition_dict['states'], dtype=torch.float).to(self.device)\n",
    "        actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(self.device)\n",
    "        rewards = torch.tensor(transition_dict['rewards'], dtype=torch.float).view(-1, 1).to(self.device)\n",
    "        next_states = torch.tensor(transition_dict['next_states'], dtype=torch.float).to(self.device)\n",
    "\n",
    "        td_target = rewards + self.gamma * self.target_critics[agent_i](next_states) # 时序差分目标\n",
    "        td_delta = td_target - self.critics[agent_i](states) # 时序差分误差\n",
    "        log_probs = torch.log(self.actors[agent_i](observations).gather(1, actions))\n",
    "        actor_loss = torch.mean(-log_probs * td_delta.detach())\n",
    "        self.actor_optimizers[agent_i].zero_grad()\n",
    "        actor_loss.backward() # 计算策略网络的梯度\n",
    "        self.actor_optimizers[agent_i].step() # 更新策略网络的参数\n",
    "        self.actor_schedulers[agent_i].step() # 更新学习率\n",
    "\n",
    "    def critic_update(self, agent_i, transition_dict):\n",
    "        states = torch.tensor(transition_dict['states'], dtype=torch.float).to(self.device)\n",
    "        rewards = torch.tensor(transition_dict['rewards'], dtype=torch.float).view(-1, 1).to(self.device)\n",
    "        next_states = torch.tensor(transition_dict['next_states'], dtype=torch.float).to(self.device)\n",
    "\n",
    "        td_target = rewards + self.gamma * self.target_critics[agent_i](next_states) # 时序差分目标\n",
    "        critic_loss = torch.mean(F.mse_loss(self.critics[agent_i](states), td_target.detach())) # 均方误差损失函数\n",
    "        self.critic_optimizers[agent_i].zero_grad()\n",
    "        critic_loss.backward() # 计算价值网络的梯度\n",
    "        self.critic_optimizers[agent_i].step() # 更新价值网络的参数\n",
    "        self.critic_schedulers[agent_i].step()\n",
    "        self.soft_update(self.critics[agent_i], self.target_critics[agent_i])  # 软更新目标价值网络\n",
    "\n",
    "    def save(self, file):\n",
    "        for i in range(self.agent_n):\n",
    "            torch.save(self.actors[i].state_dict(), file + 'actor'+str(i)+'.params')\n",
    "            torch.save(self.critics[i].state_dict(), file + 'critic'+str(i)+'.params')\n",
    "\n",
    "    def load(self, file):\n",
    "        for i in range(self.agent_n):\n",
    "            self.actors[i].load_state_dict(torch.load(file + 'actor'+str(i)+'.params'))\n",
    "            self.critics[i].load_state_dict(torch.load(file + 'critic'+str(i)+'.params'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish load grids! 49\n",
      "Finish load nodes! 11668 710\n",
      "Finish construct taxis! 6000\n",
      "Finish load trips! 225532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 0: 100%|██████████| 20/20 [03:51<00:00, 11.58s/it, episode=20, return=1719.01, response_rate=0.8419, response_time=162.25, occupied_rate=0.3657, gmv=57.50]\n",
      "Iteration 1: 100%|██████████| 20/20 [03:45<00:00, 11.29s/it, episode=40, return=1876.56, response_rate=0.8978, response_time=111.02, occupied_rate=0.3891, gmv=61.17]\n",
      "Iteration 2: 100%|██████████| 20/20 [03:33<00:00, 10.67s/it, episode=60, return=2045.85, response_rate=0.9430, response_time=60.06, occupied_rate=0.4076, gmv=64.07]\n",
      "Iteration 3: 100%|██████████| 20/20 [03:22<00:00, 10.11s/it, episode=80, return=2174.55, response_rate=0.9670, response_time=29.75, occupied_rate=0.4172, gmv=65.55]\n",
      "Iteration 4: 100%|██████████| 20/20 [03:27<00:00, 10.39s/it, episode=100, return=2243.13, response_rate=0.9764, response_time=15.97, occupied_rate=0.4208, gmv=66.12]\n",
      "Iteration 5: 100%|██████████| 20/20 [03:29<00:00, 10.46s/it, episode=120, return=2269.67, response_rate=0.9791, response_time=10.95, occupied_rate=0.4218, gmv=66.27]\n",
      "Iteration 6: 100%|██████████| 20/20 [03:24<00:00, 10.20s/it, episode=140, return=2279.43, response_rate=0.9797, response_time=9.64, occupied_rate=0.4220, gmv=66.30]\n",
      "Iteration 7: 100%|██████████| 20/20 [03:27<00:00, 10.39s/it, episode=160, return=2286.63, response_rate=0.9801, response_time=8.79, occupied_rate=0.4222, gmv=66.32]\n",
      "Iteration 8: 100%|██████████| 20/20 [03:23<00:00, 10.18s/it, episode=180, return=2293.77, response_rate=0.9804, response_time=8.24, occupied_rate=0.4223, gmv=66.33]\n",
      "Iteration 9: 100%|██████████| 20/20 [03:18<00:00,  9.92s/it, episode=200, return=2291.52, response_rate=0.9804, response_time=8.16, occupied_rate=0.4223, gmv=66.33]\n",
      "Iteration 10: 100%|██████████| 20/20 [03:27<00:00, 10.36s/it, episode=220, return=2297.95, response_rate=0.9805, response_time=8.03, occupied_rate=0.4223, gmv=66.34]\n",
      "Iteration 11: 100%|██████████| 20/20 [03:22<00:00, 10.14s/it, episode=240, return=2295.62, response_rate=0.9806, response_time=7.99, occupied_rate=0.4223, gmv=66.35]\n",
      "Iteration 12: 100%|██████████| 20/20 [03:26<00:00, 10.31s/it, episode=260, return=2297.24, response_rate=0.9806, response_time=8.07, occupied_rate=0.4223, gmv=66.34]\n",
      "Iteration 13: 100%|██████████| 20/20 [03:25<00:00, 10.25s/it, episode=280, return=2295.71, response_rate=0.9806, response_time=7.94, occupied_rate=0.4224, gmv=66.35]\n",
      "Iteration 14: 100%|██████████| 20/20 [03:25<00:00, 10.28s/it, episode=300, return=2296.41, response_rate=0.9806, response_time=7.93, occupied_rate=0.4223, gmv=66.34]\n",
      "Iteration 15: 100%|██████████| 20/20 [03:24<00:00, 10.22s/it, episode=320, return=2299.33, response_rate=0.9806, response_time=8.07, occupied_rate=0.4223, gmv=66.35]\n",
      "Iteration 16: 100%|██████████| 20/20 [03:27<00:00, 10.40s/it, episode=340, return=2295.67, response_rate=0.9806, response_time=7.97, occupied_rate=0.4223, gmv=66.34]\n",
      "Iteration 17: 100%|██████████| 20/20 [03:27<00:00, 10.39s/it, episode=360, return=2298.20, response_rate=0.9806, response_time=7.96, occupied_rate=0.4224, gmv=66.35]\n",
      "Iteration 18: 100%|██████████| 20/20 [03:26<00:00, 10.32s/it, episode=380, return=2295.33, response_rate=0.9806, response_time=7.98, occupied_rate=0.4224, gmv=66.35]\n",
      "Iteration 19: 100%|██████████| 20/20 [03:26<00:00, 10.33s/it, episode=400, return=2299.27, response_rate=0.9806, response_time=7.99, occupied_rate=0.4224, gmv=66.35]\n",
      "Iteration 20: 100%|██████████| 20/20 [03:25<00:00, 10.28s/it, episode=420, return=2296.69, response_rate=0.9806, response_time=8.03, occupied_rate=0.4223, gmv=66.35]\n",
      "Iteration 21: 100%|██████████| 20/20 [03:27<00:00, 10.36s/it, episode=440, return=2298.87, response_rate=0.9807, response_time=7.93, occupied_rate=0.4224, gmv=66.35]\n",
      "Iteration 22: 100%|██████████| 20/20 [03:25<00:00, 10.28s/it, episode=460, return=2296.83, response_rate=0.9806, response_time=7.98, occupied_rate=0.4224, gmv=66.35]\n",
      "Iteration 23: 100%|██████████| 20/20 [03:27<00:00, 10.36s/it, episode=480, return=2297.66, response_rate=0.9807, response_time=7.95, occupied_rate=0.4224, gmv=66.35]\n",
      "Iteration 24: 100%|██████████| 20/20 [03:23<00:00, 10.17s/it, episode=500, return=2297.44, response_rate=0.9806, response_time=8.06, occupied_rate=0.4223, gmv=66.35]\n",
      "Iteration 25: 100%|██████████| 20/20 [03:08<00:00,  9.42s/it, episode=520, return=2296.61, response_rate=0.9805, response_time=8.10, occupied_rate=0.4224, gmv=66.35]\n",
      "Iteration 26: 100%|██████████| 20/20 [03:24<00:00, 10.25s/it, episode=540, return=2298.10, response_rate=0.9806, response_time=8.01, occupied_rate=0.4223, gmv=66.34]\n",
      "Iteration 27: 100%|██████████| 20/20 [03:24<00:00, 10.23s/it, episode=560, return=2299.00, response_rate=0.9807, response_time=7.96, occupied_rate=0.4224, gmv=66.35]\n",
      "Iteration 28: 100%|██████████| 20/20 [03:25<00:00, 10.29s/it, episode=580, return=2298.37, response_rate=0.9806, response_time=7.97, occupied_rate=0.4224, gmv=66.35]\n",
      "Iteration 29: 100%|██████████| 20/20 [03:23<00:00, 10.18s/it, episode=600, return=2295.67, response_rate=0.9806, response_time=8.10, occupied_rate=0.4223, gmv=66.35]\n",
      "Iteration 30: 100%|██████████| 20/20 [03:17<00:00,  9.87s/it, episode=620, return=2295.97, response_rate=0.9806, response_time=8.07, occupied_rate=0.4223, gmv=66.35]\n",
      "Iteration 31: 100%|██████████| 20/20 [03:22<00:00, 10.12s/it, episode=640, return=2296.52, response_rate=0.9806, response_time=8.04, occupied_rate=0.4223, gmv=66.35]\n",
      "Iteration 32: 100%|██████████| 20/20 [03:28<00:00, 10.42s/it, episode=660, return=2297.91, response_rate=0.9805, response_time=8.06, occupied_rate=0.4223, gmv=66.35]\n",
      "Iteration 33: 100%|██████████| 20/20 [03:21<00:00, 10.06s/it, episode=680, return=2298.12, response_rate=0.9806, response_time=8.13, occupied_rate=0.4224, gmv=66.35]\n",
      "Iteration 34: 100%|██████████| 20/20 [03:26<00:00, 10.34s/it, episode=700, return=2296.28, response_rate=0.9806, response_time=7.99, occupied_rate=0.4224, gmv=66.35]\n",
      "Iteration 35: 100%|██████████| 20/20 [03:26<00:00, 10.35s/it, episode=720, return=2297.01, response_rate=0.9806, response_time=8.15, occupied_rate=0.4223, gmv=66.34]\n",
      "Iteration 36: 100%|██████████| 20/20 [03:31<00:00, 10.56s/it, episode=740, return=2296.99, response_rate=0.9806, response_time=8.05, occupied_rate=0.4223, gmv=66.34]\n",
      "Iteration 37: 100%|██████████| 20/20 [03:22<00:00, 10.12s/it, episode=760, return=2296.26, response_rate=0.9806, response_time=8.06, occupied_rate=0.4224, gmv=66.35]\n",
      "Iteration 38: 100%|██████████| 20/20 [03:28<00:00, 10.43s/it, episode=780, return=2298.73, response_rate=0.9807, response_time=7.92, occupied_rate=0.4224, gmv=66.35]\n",
      "Iteration 39: 100%|██████████| 20/20 [03:26<00:00, 10.33s/it, episode=800, return=2297.02, response_rate=0.9806, response_time=8.03, occupied_rate=0.4223, gmv=66.35]\n",
      "Iteration 40: 100%|██████████| 20/20 [03:26<00:00, 10.32s/it, episode=820, return=2297.13, response_rate=0.9806, response_time=8.11, occupied_rate=0.4223, gmv=66.35]\n",
      "Iteration 41: 100%|██████████| 20/20 [03:25<00:00, 10.27s/it, episode=840, return=2297.23, response_rate=0.9806, response_time=8.13, occupied_rate=0.4223, gmv=66.34]\n",
      "Iteration 42: 100%|██████████| 20/20 [03:26<00:00, 10.34s/it, episode=860, return=2295.29, response_rate=0.9805, response_time=8.10, occupied_rate=0.4223, gmv=66.34]\n",
      "Iteration 43: 100%|██████████| 20/20 [03:29<00:00, 10.46s/it, episode=880, return=2294.73, response_rate=0.9806, response_time=8.09, occupied_rate=0.4223, gmv=66.34]\n",
      "Iteration 44: 100%|██████████| 20/20 [03:27<00:00, 10.37s/it, episode=900, return=2297.14, response_rate=0.9806, response_time=8.00, occupied_rate=0.4224, gmv=66.35]\n",
      "Iteration 45: 100%|██████████| 20/20 [03:34<00:00, 10.74s/it, episode=920, return=2296.80, response_rate=0.9806, response_time=8.10, occupied_rate=0.4223, gmv=66.35]\n",
      "Iteration 46: 100%|██████████| 20/20 [03:23<00:00, 10.16s/it, episode=940, return=2297.54, response_rate=0.9805, response_time=8.17, occupied_rate=0.4223, gmv=66.34]\n",
      "Iteration 47: 100%|██████████| 20/20 [03:27<00:00, 10.40s/it, episode=960, return=2295.24, response_rate=0.9806, response_time=8.03, occupied_rate=0.4224, gmv=66.35]\n",
      "Iteration 48: 100%|██████████| 20/20 [03:25<00:00, 10.25s/it, episode=980, return=2299.77, response_rate=0.9806, response_time=8.11, occupied_rate=0.4223, gmv=66.35]\n",
      "Iteration 49: 100%|██████████| 20/20 [03:26<00:00, 10.35s/it, episode=1000, return=2296.30, response_rate=0.9806, response_time=8.05, occupied_rate=0.4224, gmv=66.35]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1652.9439398990653, 1806.0940977248322, 1969.0798147914065, 2124.576994860223, 2212.1116232832433, 2263.950209900352, 2283.903283560456, 2279.8181830141107, 2297.3444024087953, 2289.7862626886713, 2288.93545383476, 2291.295162239753, 2294.822382742729, 2295.1905562784686, 2296.404239828322, 2294.7511384610843, 2297.6092902143314, 2290.322156296516, 2294.643832542076, 2292.721592300613, 2282.013994693593, 2298.0213763112833, 2297.4986205639166, 2297.22695912128, 2290.933818615144, 2301.160170472053, 2289.079961118181, 2299.754456511798, 2300.1352277808633, 2290.3905418966488, 2296.1468049387195, 2304.3914688686264, 2305.7955364940835, 2298.3256823279085, 2301.393859041922, 2298.5882792715884, 2303.9426398262663, 2285.7029318452196, 2298.3477187699627, 2305.6200688003332, 2300.8977017652765, 2299.401489647833, 2293.4183373552687, 2304.451060678657, 2290.7480331953043, 2296.111198875844, 2305.2829827833866, 2295.98568976584, 2300.6024435648246, 2295.6091946195884]\n",
      "[0.8178801483505759, 0.872686458573634, 0.9261396908771494, 0.9581166929888382, 0.9731070218089543, 0.9785237698081735, 0.9797348854541905, 0.9798945930119071, 0.9804580057849627, 0.9802406260536263, 0.98063545862687, 0.9803470977587706, 0.980843965716111, 0.98027168030096, 0.9807330576899189, 0.9805201142796302, 0.9807951661845864, 0.9804846237112488, 0.9805245506006779, 0.9806532039110606, 0.9804092062534382, 0.9806443312689652, 0.9805245506006779, 0.980480187390201, 0.9805334232427733, 0.9803071708693415, 0.9803648430429613, 0.9806798218373467, 0.9805467322059163, 0.9805201142796302, 0.9806177133426792, 0.9803870246481997, 0.9806443312689652, 0.9808040388266818, 0.9805467322059163, 0.9804979326743918, 0.9805910954163931, 0.9805068053164872, 0.9810036732738274, 0.9804846237112488, 0.9804846237112488, 0.9805999680584885, 0.9805822227742977, 0.9805201142796302, 0.9807774209003958, 0.9804979326743918, 0.9806088407005839, 0.9803204798324845, 0.9806088407005839, 0.9806886944794421]\n",
      "[181.30933579401275, 135.0598903341437, 80.49952974996894, 40.89737902152503, 21.25601121501961, 12.220023778680815, 9.749880219331713, 9.392933827835252, 8.225205401664507, 8.474349191702306, 7.891150426774085, 8.280570688339573, 7.714939754760173, 8.325554983763064, 7.900732880237077, 8.001881000124216, 7.788671410572641, 8.297872340425531, 8.190868276755452, 8.216421485990098, 8.403811687044168, 7.7538019271378635, 7.94917750607776, 8.095576100651252, 8.030096001987472, 8.11580572462868, 8.482334569588131, 7.81528933685873, 8.1102159601086, 8.36574805245506, 8.159192944475006, 8.355633240466346, 7.860273632282221, 8.117402800205845, 8.037549021347576, 8.478075701382357, 7.974996894575266, 8.215622948201515, 7.773233013326708, 8.349511117420546, 8.31224602062002, 8.275779461608078, 8.03834755913616, 8.02370769967881, 8.021844444838784, 8.081468599719624, 8.124057281777368, 8.592798963675403, 8.037282842084716, 7.855748584813586]\n",
      "[0.35552198302469135, 0.37855617283950616, 0.4006596450617284, 0.41388359567901234, 0.4195159915123457, 0.4216077700617284, 0.4220163464506173, 0.4221913811728395, 0.42225506944444446, 0.42216073302469137, 0.42232940200617286, 0.42220600308641976, 0.4223927276234568, 0.4222373688271605, 0.4224001466049383, 0.42237162808641976, 0.42244021990740743, 0.42225180941358026, 0.42232764660493827, 0.4223473842592593, 0.4222656558641975, 0.4224096913580247, 0.4223561689814815, 0.42228883487654323, 0.42231721836419756, 0.4222797337962963, 0.4222428240740741, 0.4223493209876543, 0.4223295910493827, 0.4223348649691358, 0.42219568287037035, 0.42230413194444444, 0.42243074459876545, 0.42238171682098763, 0.4222656635802469, 0.42228231095679014, 0.4223424112654321, 0.42229502700617283, 0.4224907677469136, 0.42225029706790124, 0.4222702276234568, 0.4223050810185185, 0.42242192901234565, 0.422352287808642, 0.42242590277777775, 0.42234868441358026, 0.4223732137345679, 0.42227084104938273, 0.4223508912037037, 0.4224249652777778]\n",
      "[55.91703013888793, 59.542245972220776, 62.99366499999886, 65.0368231944438, 65.91787569444374, 66.23375416666585, 66.29451624999926, 66.3358805555548, 66.33982430555479, 66.3195158333326, 66.34467597222154, 66.33059319444368, 66.35301527777699, 66.32846847222146, 66.3497891666659, 66.34863833333274, 66.34968902777702, 66.31946041666595, 66.34733027777717, 66.3579968055549, 66.33385999999932, 66.34615180555483, 66.32867208333255, 66.33472666666599, 66.34086319444371, 66.33932541666606, 66.32811138888809, 66.35781194444388, 66.34945388888816, 66.34934041666605, 66.33064374999918, 66.32046472222147, 66.3588091666658, 66.35515388888818, 66.3402920833328, 66.34296291666608, 66.35057805555488, 66.3523779166659, 66.36967249999944, 66.33202499999926, 66.33040680555482, 66.32987236111038, 66.3468618055548, 66.33579694444393, 66.35525486111037, 66.35867013888814, 66.35629138888832, 66.33586763888817, 66.35417097222154, 66.35881319444377]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import simulator\n",
    "import os\n",
    "\n",
    "# 训练\n",
    "def train_on_policy_agent():\n",
    "    return_list = [[], [], [], [], []]\n",
    "    for i in range(num_episodes // num_show):\n",
    "        with tqdm(total=num_show, desc='Iteration %d' % i) as pbar:\n",
    "            for i_episode in range(num_show):\n",
    "                episode_return = 0\n",
    "                transition_dict = [{'observations': [],'states': [], 'actions': [], 'next_states': [], 'rewards': []} for _ in range(env.grid_number)]\n",
    "                critic_transition_dict = [{'states': [], 'next_states': [], 'rewards': []} for _ in range(env.grid_number)]\n",
    "                observations, state = env.reset()\n",
    "                done = False\n",
    "                while not done:\n",
    "                    for k in range(env.action_dim):\n",
    "                        for j in range(env.grid_number):\n",
    "                            transition_dict[j]['observations'].append(observations[j].copy()) # .copy() 很重要\n",
    "                            transition_dict[j]['states'].append(state.copy())\n",
    "                    for j in range(env.grid_number):\n",
    "                        critic_transition_dict[j]['states'].append(state.copy())\n",
    "\n",
    "                    actions = agents.take_action(observations)\n",
    "                    observations, state, raw_reward, rewards, done = env.step(actions)\n",
    "\n",
    "                    for k in range(env.action_dim):\n",
    "                        for j in range(env.grid_number):\n",
    "                            transition_dict[j]['actions'].append(k)\n",
    "                            transition_dict[j]['next_states'].append(state.copy())\n",
    "                            transition_dict[j]['rewards'].append(rewards[j][k])\n",
    "                    critic_reward = 0\n",
    "                    for j in range(env.grid_number):\n",
    "                        critic_reward += raw_reward[j]\n",
    "                        critic_transition_dict[j]['next_states'].append(state.copy())\n",
    "                        critic_transition_dict[j]['rewards'].append(raw_reward[j])\n",
    "                    \n",
    "                    episode_return += critic_reward\n",
    "                return_list[0].append(episode_return)\n",
    "                response_rate, response_time, occupied_rate, gmv = env.rate_view()\n",
    "                return_list[1].append(response_rate)\n",
    "                return_list[2].append(response_time)\n",
    "                return_list[3].append(occupied_rate)\n",
    "                return_list[4].append(gmv)\n",
    "                \n",
    "                for j in range(env.grid_number):\n",
    "                    agents.critic_update(j, critic_transition_dict[j])\n",
    "                    agents.actor_update(j, transition_dict[j])\n",
    "                if (i_episode+1) % num_show == 0:\n",
    "                    pbar.set_postfix({\"episode\": \"%d\" % (num_show * i + i_episode+1), \"return\": \"%.2f\" % np.mean(return_list[0][-num_show:]), \"response_rate\": \"%.4f\" % np.mean(return_list[1][-num_show:]), \"response_time\": \"%.2f\" %np.mean(return_list[2][-num_show:]), \"occupied_rate\": \"%.4f\" %np.mean(return_list[3][-num_show:]), \"gmv\": \"%.2f\" %np.mean(return_list[4][-num_show:])})\n",
    "                pbar.update(1)\n",
    "    \n",
    "    # 参数的变化趋势\n",
    "    for i in range(len(return_list)):\n",
    "        rt = []\n",
    "        for j in range(num_episodes):\n",
    "            if j % num_show == 0:\n",
    "                rt.append(return_list[i][j])\n",
    "        print(rt)\n",
    "\n",
    "\n",
    "actor_lr = 1e-4 # 学习率\n",
    "critic_lr = 1e-3\n",
    "num_episodes = 1000\n",
    "num_show = 20 # 打印结果时每组的轮数\n",
    "hidden_dim = 256\n",
    "tau = 0.005 # 软更新参数\n",
    "gamma = 0.9 # 折扣因子\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "env_param = simulator.CityParam() # 环境参数\n",
    "env_param.file_path1 = os.path.join(os.path.dirname('__file__'), '../data/shanghai/2015-04-04/')\n",
    "env_param.file_path2 = os.path.join(os.path.dirname('__file__'), '../data/shanghai/2500m/')\n",
    "env_param.taxi_number = 6000\n",
    "env_param.simulated_start_time = 8*60*60\n",
    "env_param.simulated_end_time = 20*60*60\n",
    "env = simulator.ManyToManyManner(env_param) # 生成环境\n",
    "\n",
    "torch.manual_seed(0) # 设置随机数种子，方便复现\n",
    "agent_n = env.grid_number\n",
    "observation_dim = env.observation_dim\n",
    "state_dim = env.state_dim\n",
    "action_dim = env.action_dim\n",
    "agents = MutiActorCritic(agent_n, observation_dim, state_dim, hidden_dim, action_dim, actor_lr, critic_lr, tau, gamma, device)\n",
    "\n",
    "train_on_policy_agent()\n",
    "model_param_file = os.path.join(os.path.dirname('__file__'), '../data/model-param/shanghai-2015-04-04-2500m/MAN-A2C/')\n",
    "agents.save(model_param_file) # 保存模型\n",
    "\n",
    "env.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载现有模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish load grids! 49\n",
      "Finish load nodes! 11668 710\n",
      "Finish construct taxis! 2000\n",
      "Finish load trips! 225532\n",
      "1105.577184490375 0.5508757297748124 355.5470871116001 0.7151373726851852 112.72985208333242\n",
      "1090.1988900064594 0.5498287580075595 355.72276542508826 0.7171296527777777 112.9885595833316\n",
      "1090.033597642055 0.548276045640871 356.56974783951165 0.714125613425926 112.60811333333123\n",
      "1097.7454846623127 0.5502768264333753 355.59979060564655 0.7172885416666667 112.99773999999825\n",
      "1091.499727203479 0.5507914396749064 355.20158642840664 0.7176953009259259 113.11062249999827\n",
      "1093.4889203127595 0.5512217628165315 354.93753659964864 0.7179322453703704 113.17723291666611\n",
      "1108.1444852094514 0.5533378879562756 353.9936649335439 0.720612337962963 113.52408624999798\n",
      "1098.6423931495672 0.5531870530406544 354.0572817773677 0.7196575578703703 113.42191708333134\n",
      "1099.4519553164755 0.5525925860202651 354.40704132876687 0.7192250925925926 113.44540583333186\n",
      "1089.2687962432005 0.545614253012262 358.08271076961296 0.7104855787037037 112.01213333333155\n",
      "ave:  1096.4051434236135 0.5506002342377513 355.4119212819193 0.7169289293981482 113.00156629166504\n"
     ]
    }
   ],
   "source": [
    "import simulator\n",
    "import os\n",
    "\n",
    "actor_lr = 1e-4 # 学习率\n",
    "critic_lr = 1e-3\n",
    "hidden_dim = 256\n",
    "tau = 0.005 # 软更新参数\n",
    "gamma = 0.9 # 折扣因子\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "env_param = simulator.CityParam() # 环境参数\n",
    "env_param.file_path1 = os.path.join(os.path.dirname('__file__'), '../data/shanghai/2015-04-04/')\n",
    "env_param.file_path2 = os.path.join(os.path.dirname('__file__'), '../data/shanghai/2500m/')\n",
    "env_param.taxi_number = 2000\n",
    "env_param.simulated_start_time =8*60*60\n",
    "env_param.simulated_end_time = 20*60*60\n",
    "env = simulator.ManyToManyManner(env_param) # 生成环境\n",
    "\n",
    "agent_n = env.grid_number\n",
    "observation_dim = env.observation_dim\n",
    "state_dim = env.state_dim\n",
    "action_dim = env.action_dim\n",
    "agents = MutiActorCritic(agent_n, observation_dim, state_dim, hidden_dim, action_dim, actor_lr, critic_lr, tau, gamma, device)\n",
    "model_param_file = os.path.join(os.path.dirname('__file__'), '../data/model-param/shanghai-2015-04-04-2500m/MAN-A2C/')\n",
    "agents.load(model_param_file) # 加载模型\n",
    "\n",
    "N = 10\n",
    "N_total_reward = 0\n",
    "N_response_rate = 0\n",
    "N_response_time = 0\n",
    "N_occupied_rate = 0\n",
    "N_gmv = 0\n",
    "for _ in range(N):\n",
    "    observations, state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while done == False:\n",
    "        actions = agents.take_action(observations)\n",
    "        observations, state, raw_reward, rewards, done = env.step(actions)\n",
    "        for r in raw_reward:\n",
    "            total_reward += r\n",
    "    response_rate, response_time, occupied_rate, gmv = env.rate_view()\n",
    "    N_total_reward += total_reward\n",
    "    N_response_rate += response_rate\n",
    "    N_response_time += response_time\n",
    "    N_occupied_rate += occupied_rate\n",
    "    N_gmv += gmv\n",
    "    print(total_reward, response_rate, response_time, occupied_rate, gmv)\n",
    "\n",
    "print('ave: ', N_total_reward / N, N_response_rate / N, N_response_time / N, N_occupied_rate / N, N_gmv / N)\n",
    "\n",
    "env.clear()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_taxi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
